{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4fad24-7943-4e16-937a-8fb69a151439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import timeit\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "def toc(start_time):\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import xmlschema\n",
    "from pprint import pprint\n",
    "import glob\n",
    "# importing element tree\n",
    "import lxml.etree as etree\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 8, 'font.style': 'normal', 'font.family':'serif'})\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    plt.xticks(fontsize=ff-4)\n",
    "    plt.yticks(fontsize=ff-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949f8c52-ca6a-49f5-b76d-9b54b6fbc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()\n",
    "path_data=\"/mnt/sdb1/sandeep/openalex_ACTIV/\"\n",
    "path_career=path_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bb492-2338-4168-b1dd-05e4b37faefd",
   "metadata": {},
   "source": [
    "# Loading paper-author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacfd020-ac71-49a6-9762-62deec1ba732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.32244236767292\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "# names=path_data+'openalex_ACTIV/pap_auth_wc20/*'\n",
    "names=path_data+'2paperauthors_nopreprints_filtered_teamsize10_wc20and400_wcperyearexists/*'\n",
    "# names=path_data+'openalex_ACTIV/2authorspapers_statphy/*'\n",
    "\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=15)\n",
    "df_new_by_papers = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "p.close()\n",
    "df_new_by_papers.columns=['a','b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362ac7c-938d-4182-8f27-2521c815b7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4069cba1-f357-4ce3-a12e-6710463fa71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_auth_to_paper=df_new_by_papers.groupby('a')['b'].apply(list).to_dict()\n",
    "with open(path_career+'dict_auth_to_paper(whole).pkl', 'wb') as f:\n",
    "    pickle.dump(dict_auth_to_paper, f)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58edc991-ef50-4e19-ba24-42e9712e83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_paper_to_auth=df_new_by_papers.groupby('b')['a'].apply(list).to_dict()\n",
    "with open(path_career+'dict_paper_to_auth(whole).pkl', 'wb') as f:\n",
    "    pickle.dump(dict_paper_to_auth, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6863cd92-3afb-4e2a-aa4a-66d8cc6a268d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105459957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_paper_to_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073a733-4d74-4cbb-a645-77a417a87bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_paper_to_auth(whole).pkl', 'wb') as f:\n",
    "    pickle.dump(dict_paper_to_auth, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7e0469-4ca6-43c4-a84c-e365da7c28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "# with open(path_career+'dict_auth_N_ord3.pkl', 'rb') as f:\n",
    "#     dict_auth_N_ord3=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0016c434-57ef-46fb-9de6-b1e43e852ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ord3=pd.read_csv(path_career+'2auth_ord3_papercount.csv') \n",
    "dict_auth_N_ord3=df_ord3.set_index('aid1')['f0_'].T.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd446f2a-2b87-4c2c-acf5-9b89281ec899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4004817"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_auth_N_ord3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0246e2c-211c-4800-92a0-de49345b5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path_career+'dict_authwc_after_year_x.pkl', 'rb') as f:\n",
    "    dict_authwc_after_year_x=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9a3931-22dc-4131-80fc-42030bf349bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000926"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_authwc_after_year_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fef6233-3610-4c91-887f-b717b200f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_authwc_after_year_x['https://openalex.org/A2764379743']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa25b-059d-435d-8119-e918b3a27c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e441d41-7a35-4190-bf30-f378623e7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_firstpub.pkl', 'rb') as f:\n",
    "    dict_firstpub=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d4f085-763d-4e14-aab0-49f1eb608017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_firstpub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf0b39-e823-4db2-a607-c9cd3995a555",
   "metadata": {},
   "source": [
    "# Run svs to detect significant higher-order interactions (i.e. persistent teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ba8c3e-9bd4-459d-8d85-1498faa69fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter,defaultdict,OrderedDict\n",
    "from itertools import combinations\n",
    "import numpy as np \n",
    "from multiprocessing import Pool,cpu_count\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import binom\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import time\n",
    "sns.set_context('talk')#, font_scale=1.5)\n",
    "import matplotlib as mpl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "def p_over(t):\n",
    "    w,n,na,nb = t\n",
    "    return st.hypergeom.sf(w-1,n,na,nb)\n",
    "\n",
    "def p_appr(t):\n",
    "    n12 = t[0]\n",
    "    n = t[1]\n",
    "    ns = np.array(t[2:])\n",
    "    order = len(ns)\n",
    "    p = st.binom.sf(n12-1,p=np.prod(ns/n),n=n)\n",
    "    if p>=0:\n",
    "        return p\n",
    "    else:\n",
    "        print(n12,n,ns)\n",
    "        return p\n",
    "    \n",
    "    \n",
    "def tpr(true,pred):\n",
    "    return len(set(pred).intersection(true))/len(true)\n",
    "\n",
    "def fdr(true,pred):\n",
    "    try:\n",
    "        return len(set(pred).difference(true))/len(pred)\n",
    "    except: return np.nan\n",
    "def jaccard(true,pred):\n",
    "    return len(set(true).intersection(pred))/len(set(true).union(pred))\n",
    "\n",
    "def expand(x,order):\n",
    "    return tuple(combinations(x,order))\n",
    "\n",
    "def expand_filter(x,order,drop):\n",
    "    return list(set(combinations(x,order)).difference(drop))\n",
    "\n",
    "def tuple_to_validate(x,groups,N,deg_a):\n",
    "    return tuple([groups[x],N])+tuple([deg_a[ii] for ii in x])\n",
    "\n",
    "\n",
    "def get_svs(df,dict_auth_N_ord3,dict_authwc_after_year_x,dict_firstpub,min_order=2,max_order=10,approximate=True):\n",
    "\n",
    "\n",
    "    observables = df.groupby('b')['a'].apply(lambda x: tuple(sorted(x))).tolist()\n",
    "    print(len(observables))\n",
    "\n",
    "    if max_order!=0:\n",
    "        max_order = min(max_order,max(map(len,observables)))\n",
    "    else:\n",
    "        max_order = max(map(len,observables))\n",
    "    #     max_order=10\n",
    "    print(max_order)\n",
    "\n",
    "    s_groups = []\n",
    "\n",
    "    neigh_set_a_sub = dict(df.groupby('a')['b'].apply(list).reset_index().values) \n",
    "    N = df.b.nunique()\n",
    "    na = df.a.nunique()\n",
    "\n",
    "    if approximate: deg_a = Counter(df.a)\n",
    "\n",
    "    significant_cores3 = []\n",
    "    nonsignificant_cores3 = []\n",
    "    groups_higher_order = {}\n",
    "\n",
    "    t_ic = time.time();\n",
    "    for order in list(range(min_order,max_order+1))[::-1]:\n",
    "    # for order in range(4,5):\n",
    "        t_oc = time.time();\n",
    "        print(order, '---- ',str(round(t_oc-t_ic,2)))\n",
    "\n",
    "        expand_order = partial(expand,order=order) \n",
    "\n",
    "        order_obs = filter(lambda x: len(x)>=order,s_groups)\n",
    "        #p = Pool(processes=cpu_count())\n",
    "        B = (i for ii in map(expand_order,order_obs) for i in ii)\n",
    "        drop = Counter(B)\n",
    "        #p.close()\n",
    "\n",
    "        expand_filter_order = partial(expand_filter,order=order,drop=drop)\n",
    "        print('drop')\n",
    "        if not approximate:\n",
    "\n",
    "            groups = set()\n",
    "            for l in (map(lambda x: tuple(combinations(x,order)), filter(lambda x: len(x)>=order,observables))): \n",
    "                for g in l: groups.add(g)\n",
    "            groups = groups.difference(drop)\n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "            order_obs = filter(lambda x: len(x)>=order,observables)\n",
    "            #p = Pool(processes=cpu_count())\n",
    "            B = (i for ii in map(expand_order,order_obs) for i in ii)\n",
    "            groups = Counter(B)\n",
    "\n",
    "\n",
    "\n",
    "            for g in drop: del groups[g]\n",
    "\n",
    "\n",
    "                # MAKE DICTIONARY\n",
    "                # YEAR_YOUNGEST=np.max ([dict_first_paper_year[x] for x in g])\n",
    "\n",
    "\n",
    "        print(len(groups))\n",
    "        print('dropped')\n",
    "\n",
    "\n",
    "        dict_YEAR_YOUNGEST_START={}\n",
    "        t_ic = time.time();\n",
    "        it_auth=0\n",
    "        for auths in list(groups.keys()):\n",
    "            dict_YEAR_YOUNGEST_START[auths]= np.max([dict_firstpub[auth] for auth in auths])\n",
    "            it_auth+=1\n",
    "            if (it_auth+1)%1000==0:\n",
    "                t_oc = time.time();\n",
    "                frac=it_auth/len(groups)\n",
    "                prog=str(frac)+str(round(t_oc-t_ic,2))+', time estimate: '+str(round((t_oc-t_ic)/frac/3600,2))\n",
    "                print(prog,end='\\r')\n",
    "\n",
    "\n",
    "        #print(len(groups))\n",
    "        p = Pool(processes=cpu_count())\n",
    "    #         p = Pool(processes=8)\n",
    "        if not approximate:\n",
    "\n",
    "            pvalues = dict(zip(groups,p.map(pvalue_intersect,zip(groups,[neigh_set_a_sub]*len(groups),[N]*len(groups)))))\n",
    "        else:\n",
    "    #             pvalues = dict(zip(groups,p.map(p_appr,[tuple([groups[i],sum([dict_auth_N_ord3[x] for x in i])])+tuple([deg_a[ii] for ii in i]) for i in groups])))\n",
    "            pvalues = dict(zip(groups,p.map(p_appr,[tuple([groups[i],sum([dict_auth_N_ord3.get(x,0) for x in i])])+tuple([dict_authwc_after_year_x[ii].get(round(dict_YEAR_YOUNGEST_START[i],0),0) for ii in i]) for i in groups])))\n",
    "\n",
    "\n",
    "            # max_first_year: groups\n",
    "\n",
    "    #     dict_authworkcount_after_year={'openalex/a21312312':{'1997':28,......}}\n",
    "    #  modify it as follows---- dict_authworkcount_after_year\n",
    "\n",
    "\n",
    "            #tuple_to_validate_order = partial(tuple_to_validate,groups=groups,N=N,deg_a=deg_a)\n",
    "            #params = p.map(tuple_to_validate_order,groups)\n",
    "            #pvalues = dict(zip(groups,p.map(p_appr,params)))\n",
    "\n",
    "        p.close()\n",
    "\n",
    "        n_possible = binom(na,order)\n",
    "        bonf = 0.01/n_possible\n",
    "\n",
    "        temp_df = pd.DataFrame(pvalues.items())\n",
    "        #print(temp_df)\n",
    "        try:\n",
    "            temp_df.columns = ['group','pvalue']\n",
    "        except: temp_df = pd.DataFrame(columns=['group','pvalue'])\n",
    "        #print(temp_df.pvalue.min())\n",
    "        ps = np.sort(temp_df.pvalue)\n",
    "        k = np.arange(1,len(ps)+1)*bonf\n",
    "        try: fdr = k[ps<k][-1] \n",
    "        except: fdr = 0\n",
    "        if approximate: temp_df['w'] = temp_df.group.apply(lambda x: groups[x])\n",
    "        temp_df['fdr'] = temp_df['pvalue']<bonf\n",
    "    #     temp_df['ni'] = temp_df['group'].apply(lambda x: tuple([deg_a[ii] for ii in x]))\n",
    "        temp_df['ni'] = temp_df['group'].apply(lambda x: tuple([dict_authwc_after_year_x[ii].get(round(dict_YEAR_YOUNGEST_START[x],0),list(dict_authwc_after_year_x[ii].items())[0][1]) for ii in x]))\n",
    "\n",
    "\n",
    "        temp_df['N'] = temp_df['group'].apply(lambda i: sum([dict_auth_N_ord3[x] for x in i]))\n",
    "    #         svh_dfs.append(temp_df.query('fdr'))\n",
    "\n",
    "        significant_cores3=temp_df.query('fdr')\n",
    "        value=False;\n",
    "        nonsignificant_cores3=temp_df.query('fdr== @value')\n",
    "        PAPERS_TOGETHER=2\n",
    "        significant_cores3_sample=significant_cores3[significant_cores3['w']>PAPERS_TOGETHER]\n",
    "        nonsignificant_cores3_sample=nonsignificant_cores3\n",
    "        s_groups_order = temp_df.query('fdr').group.tolist()\n",
    "        s_groups.extend(s_groups_order)\n",
    "\n",
    "    #     nonsignificant_cores3_sample=nonsignificant_cores3.sample(n=10*significant_cores3_sample.shape[0], random_state=12)\n",
    "\n",
    "        path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "        import pickle\n",
    "        with open(path_career+'significant_cores3_size'+str(order)+'(whole).pkl', 'wb') as f:\n",
    "            pickle.dump(significant_cores3_sample, f)    \n",
    "        with open(path_career+'significant_non_cores3_size'+str(order)+'(whole).pkl', 'wb') as f:\n",
    "            pickle.dump(nonsignificant_cores3_sample, f)    \n",
    "        print(len(groups_higher_order),temp_df.fdr.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6c707-5b51-4af0-9aea-8566905c026d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21347d1-1550-42f5-b03e-8f0a0b7c0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "# from svs2 import *\n",
    "get_svs(df_new_by_papers[['a','b']],dict_auth_N_ord3,dict_authwc_after_year_x,dict_firstpub,min_order=2,max_order=10,approximate=True)\n",
    "# all_cores5=get_svs(df_new_by_papers[['a','b']],dict_auth_N_ord5,min_order=2,max_order=0,approximate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee844ca-35a2-4ff8-a766-cc3eb91c491f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf182829-d282-462f-9f83-f7734bc4607e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930b891-b3ba-4597-8828-4bab66be4662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
