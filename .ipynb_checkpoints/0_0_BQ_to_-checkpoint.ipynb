{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de858fd-073c-4d87-a2da-886c30df9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import timeit\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "def toc(start_time):\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import xmlschema\n",
    "from pprint import pprint\n",
    "import glob\n",
    "# importing element tree\n",
    "import lxml.etree as etree\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 8, 'font.style': 'normal', 'font.family':'serif'})\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    plt.xticks(fontsize=ff-4)\n",
    "    plt.yticks(fontsize=ff-4)\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7639f-6c63-429f-af74-130c2f4581bc",
   "metadata": {},
   "source": [
    "# Load authc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abbba19-eb59-4f9f-b781-897e1c5fa5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.708442833274603\n",
      "None\n",
      "1.7094787433743477\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authc5/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_authc5=df.set_index('auth_id')['c5'].T.to_dict()\n",
    "with open(path_career+'dict_authc5.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authc5, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348eb13-fae8-4a12-aa12-71dc393fc157",
   "metadata": {},
   "source": [
    "# Literature popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4673f9a-261c-407c-b36e-f436b602f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.52010865136981\n",
      "None\n",
      "25.521927036345005\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2literaturepopularity/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "literaturepopularity=df.set_index('citing')['c5_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturepopularity.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturepopularity, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9672d-c8da-409d-bdb2-5bcac2a25544",
   "metadata": {},
   "source": [
    "# literature depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc69a4f1-3714-40f4-92c0-d9930db13a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.308283876627684\n",
      "None\n",
      "31.309843335300684\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2literaturedepth/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "literaturedepth=df.set_index('citing')['pubdate_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturedepth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturedepth, f) \n",
    "\n",
    "literaturebreadth=df.set_index('citing')['pubdate_std'].T.to_dict()\n",
    "with open(path_career+'dict_literaturebreadth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturebreadth, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccaa546-b7e7-4a3f-a96a-8ea39f0323c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "literaturedepth=df.set_index('citing')['pubdate_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturedepth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturedepth, f) \n",
    "\n",
    "literaturebreadth=df.set_index('citing')['pubdate_std'].T.to_dict()\n",
    "with open(path_career+'dict_literaturebreadth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturebreadth, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4665697-f25c-40fc-93f0-cb70fe29ecd0",
   "metadata": {},
   "source": [
    "# Team author concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090fcbd8-e17d-4c45-b108-6ebb2ff9a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# names=path_career+'2authconcepts_filteredUniqueCore_grouped.json'\n",
    "# import json\n",
    " \n",
    "# # Opening JSON file\n",
    "# with open(names) as json_file:\n",
    "#     data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65aae128-f337-4dfe-99d7-478a6c796f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authconcepts_filteredUniqueCore_grouped*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e100e44c-3e88-4fc5-b025-5c8854be98bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auth_id': 'https://openalex.org/A2120662050',\n",
       " 'concepts': ['Medicine', 'Biology', 'Chemistry', 'Psychology', 'Geology'],\n",
       " 'score': [93.3, 80, 36.7, 26.7, 26.7]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b587a2fe-465a-4839-8603-7c58c897a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0999\r"
     ]
    }
   ],
   "source": [
    "dict_author_concepts={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        \n",
    "        dict_author_concepts[res['auth_id']]={c:s/np.sum(res['score']) for c,s in zip(res['concepts'],res['score'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a68c12-f6ea-4ea0-8ce2-bb6e88a74db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path_career+'dict_author_concepts.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_author_concepts, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484965cf-4f8b-4e8c-835f-240a9c1e1fc6",
   "metadata": {},
   "source": [
    "# Team author countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552f0aa6-6582-4334-a596-c78693c2ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2paperauthaffilsgrouped/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f50272-1405-4dc0-8f4f-c38607d6dee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93d33d-9a86-4ac8-afc5-9fc31f73d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.0999\n",
      "0.3136\r"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_paper_auth_geo={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        if len(res)==3:\n",
    "            if res['paper_id'] not in dict_paper_auth_geo.keys():\n",
    "                dict_paper_auth_geo[res['paper_id']]={}\n",
    "            try:\n",
    "                dict_paper_auth_geo[res['paper_id']][res['auth_id']]=res['countries']\n",
    "            except:\n",
    "                dict_paper_auth_geo[res['paper_id']]={}\n",
    "                dict_paper_auth_geo[res['paper_id']][res['auth_id']]=res['countries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b3f1e-8dd6-4894-b4e3-1f1001c5fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_career+'dict_paper_auth_geo.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_paper_auth_geo, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b28bc-75e2-45b8-9474-55893774ef4d",
   "metadata": {},
   "source": [
    "# Team author affiliations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55f51f-bb4e-4184-88cb-0ca2e387fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authpaperuniversity_filterUniqueCorePapers_grouped/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa0eb3e-905b-4daa-936a-ac38103b5a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e279c5-67dd-4f03-91d7-be03c2f8347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_paper_auth_afill={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        if len(res)==3:\n",
    "            if res['paper_id'] not in dict_paper_auth_afill.keys():\n",
    "                dict_paper_auth_afill[res['paper_id']]={}\n",
    "            try:\n",
    "                dict_paper_auth_afill[res['paper_id']][res['auth_id']]=res['countries']\n",
    "            except:\n",
    "                dict_paper_auth_afill[res['paper_id']]={}\n",
    "                dict_paper_auth_afill[res['paper_id']][res['auth_id']]=res['countries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20809c08-dac0-4009-bfe8-d8ef96d18ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_paper_auth_afill.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_paper_auth_afill, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1727f97b-0dd9-43b4-b303-6d972fb9fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bccb3f-da7f-4352-8664-cde727a10471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2c85-0df0-4d3c-a8bb-a936926da550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "702c13e3-8a7e-48af-b2da-b8b0eef48384",
   "metadata": {},
   "source": [
    "# country to continent dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0fd5f2-eab7-47e9-9f77-af14e65c495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "\n",
    "# Create a dictionary to map alpha-2 country codes to continents\n",
    "country_to_continent = {}\n",
    "for country in pycountry.countries:\n",
    "    alpha_2_code = country.alpha_2\n",
    "    try:\n",
    "        continent_code = country_alpha2_to_continent_code(alpha_2_code)\n",
    "        # Convert continent code to continent name\n",
    "        continent_name=pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        country_to_continent[alpha_2_code] = continent_name\n",
    "    except KeyError:\n",
    "        # Some countries may not have continent information\n",
    "        pass\n",
    "\n",
    "# Example: Accessing the continent for a specific country code (e.g., 'US' for United States)\n",
    "country_code = 'US'\n",
    "if country_code in country_to_continent:\n",
    "    continent = country_to_continent[country_code]\n",
    "    \n",
    "    print(f\"The continent of {country_code} is {continent}\")\n",
    "else:\n",
    "    print(f\"Continent information not found for {country_code}\")\n",
    "\n",
    "# Example: Accessing the continent for all countries\n",
    "# for country_code, continent in country_to_continent.items():\n",
    "#     print(f\"{country_code}: {continent}\")\n",
    "\n",
    "with open(path_career+'country_to_continent.pkl', 'wb') as f:\n",
    "    pickle.dump(country_to_continent, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823c8d1-422a-442a-b45e-d6497b712a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The continent of US is North America\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2533e5-ce44-40bc-823f-83d8eb256be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883608b1-fe7d-4355-ac11-a97ef056c552",
   "metadata": {},
   "source": [
    "# Load pairs_first_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ec293-e1f4-4d4c-9f58-8e209281693f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fe8510883a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sdb1/sandeep/miniconda3/envs/sos/lib/python3.9/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authpairsfirstlastpub/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=15)\n",
    "# df_workcounts = pd.read_csv(files)\n",
    "df_pairs_firstpubtogether = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "\n",
    "# p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "# df_pairs_firstpubtogether[\"A1A2\"] = df_pairs_firstpubtogether[\"A1\"] +'+'+ df_pairs_firstpubtogether[\"A2\"]\n",
    "# dict_pairs_firstpubtogether = df_pairs_firstpubtogether.set_index('A1A2')[['first_year_together']].to_dict()\n",
    "dict_pairs_firstpubtogether = df_pairs_firstpubtogether.groupby('A1')[['A2','first_year_together','last_year_together']].apply(lambda x: x.set_index('A2').to_dict(orient='index')).to_dict()\n",
    "with open(path_career+'dict_pairs_firstpubtogether.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_pairs_firstpubtogether, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc49876-f84e-4b27-9776-23eeb4867e0c",
   "metadata": {},
   "source": [
    "# auth workcounts total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98737034-cf72-495f-afc2-ef25e396b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.81174374371767\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authworkcounts/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=3)\n",
    "# df_workcounts = pd.read_csv(files)\n",
    "df_workcounts = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "\n",
    "# p.close()\n",
    "\n",
    "print(toc(start_time))\n",
    "\n",
    "\n",
    "dict_workcounts=df_workcounts.set_index('auth_id')['wc'].T.to_dict()\n",
    "with open(path_career+'dict_workcounts.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_workcounts, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fad15-40a6-49b0-9566-c1ab89a523b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf60f31-0f90-4582-b7ba-310e7c46341a",
   "metadata": {},
   "source": [
    "# authors to papers dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec396b1-16ec-499a-85bb-f03e3e7528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authorsgroupedpapers/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# json_files = [f for f in os.listdir('.') if f.endswith('.json')]\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n",
    "\n",
    "dict_auth_to_paper={}\n",
    "for result in results:\n",
    "    for res in result:\n",
    "        dict_auth_to_paper[res['auth_id']]=res['papers']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e6e42-8d94-4b51-9801-a3c42e95b499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f2b3f-8f8f-4764-a024-4c22216e7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_auth_to_paper(whole).pkl', 'wb') as f:\n",
    "    pickle.dump(dict_auth_to_paper, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d8823-94fa-45ae-8c59-b430e3cc0a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e097ae65-ffeb-4933-a1c0-3029e13ff9a3",
   "metadata": {},
   "source": [
    "# c5\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0273b39f-901c-42ac-8d77-3c9154f3287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.530694387853146\n",
      "None\n",
      "21.531843692064285\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2c5norm/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df_c5_norm = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_c5_norm=df_c5_norm.set_index('id')['c5_normalized'].T.to_dict()\n",
    "with open(path_career+'dict_c5_norm.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_c5_norm, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616a122-5e44-477d-998d-cfa57ac85e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8550892a-dec1-47cb-95d3-5109079648bc",
   "metadata": {},
   "source": [
    "# paper pubdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f966eff-0429-4b88-a4fd-b76f832f76f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.554304394871\n",
      "None\n",
      "106.55620150640607\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2paperspubdates/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df_pubdate = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "# names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# df.to_parquet(names_par,index=None)\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "\n",
    "dict_pubdate=df_pubdate.set_index('id')['pubdate'].T.to_dict()\n",
    "with open(path_career+'dict_pubdate.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_pubdate, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8ae0f-7aae-401f-abc9-b19c154c923f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dca7977-d4f4-4002-b938-c0b4a72bf5c6",
   "metadata": {},
   "source": [
    "# disciplinary diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5507a68f-84b5-4073-8be7-1b3374c6449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.0606624558568\n",
      "None\n",
      "108.0622975602746\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2papersconceptsdiversity/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=15)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "# names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# df.to_parquet(names_par,index=None)\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_=df.set_index('paper_id')['concept_entropy'].T.to_dict()\n",
    "with open(path_career+'dict_paper_to_discidiversity.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225ecf4-9a52-4f00-a5c2-e5b02e1303d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8dd2782-715c-49c4-b123-7edc1bb0bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_=df.set_index('paper_id')['concept_entropy'].T.to_dict()\n",
    "with open(path_career+'dict_paper_to_discidiversity.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38561e2a-0e1b-47cc-a4e5-9543d980691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70abe883-3bbe-419a-9080-d39c1bc73eed",
   "metadata": {},
   "source": [
    "# c5 within discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5cb6c85-986b-453d-9df1-06376f72a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # \n",
    "# start_time = timeit.default_timer()\n",
    "# names=path_career+'c5_within_discipline/*'\n",
    "# files=sorted(glob.glob(names))\n",
    "\n",
    "# p=Pool(processes=5)\n",
    "# df_c5_within = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "# print(toc(start_time))\n",
    "# # names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# # df.to_parquet(names_par,index=None)\n",
    "\n",
    "# p.close()\n",
    "# print(toc(start_time))\n",
    "\n",
    "# dict_c5_within=df_c5_within.set_index('cited')['c5'].T.to_dict()\n",
    "# with open(path_career+'dict_c5_within.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_c5_within, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ebb16b-9526-4f1c-b41b-79de700daaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # \n",
    "# start_time = timeit.default_timer()\n",
    "# names=path_career+'c5_within_lvl1/*'\n",
    "# files=sorted(glob.glob(names))\n",
    "\n",
    "# p=Pool(processes=5)\n",
    "# df_c5_within_lvl1 = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "# print(toc(start_time))\n",
    "\n",
    "\n",
    "# p.close()\n",
    "# print(toc(start_time))\n",
    "\n",
    "# dict_c5_within_lvl1=df_c5_within_lvl1.set_index('cited')['c5'].T.to_dict()\n",
    "# with open(path_career+'dict_c5_within_lvl1.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_c5_within_lvl1, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d5ea9-003e-46a6-91e8-771a5e76568d",
   "metadata": {},
   "source": [
    "# workcount_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ea9aa8-c2ce-4d6a-b79a-ef3563cd71da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb550122-5493-4bb6-bd46-dd64604ea9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "# names=path_career+'2workcountsperyear_statphy/*'\n",
    "names=path_career+'2workcounts_per_year_for_authors/*'\n",
    "\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# json_files = [f for f in os.listdir('.') if f.endswith('.json')]\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "\n",
    "    # Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n",
    "\n",
    "dict_workcounts={}\n",
    "for result in results:\n",
    "    for res in result:\n",
    "        dict_workcounts[res['auth_id']]={int(year):int(wc) for year,wc in sorted(zip(res['pubyear'],res['wc']))}\n",
    "\n",
    "# with open(path_career+'dict_workcounts_per_year_for_authors_statphy.pkl', 'wb') as f:\n",
    "with open(path_career+'dict_workcounts_per_year_for_authors.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_workcounts, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4496ee0-4ba8-40d2-ac7c-449efc69be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_authwc_after_year_x={}\n",
    "for auth in dict_workcounts.keys():\n",
    "    dict_authwc_after_year_x[auth]={}\n",
    "    totalworks=np.sum(list(dict_workcounts[auth].values()))\n",
    "    wcount=0\n",
    "    for year in range(np.min(list(dict_workcounts[auth].keys())),2024):\n",
    "\n",
    "        dict_authwc_after_year_x[auth][year]=totalworks-wcount\n",
    "        try:\n",
    "            wcount+=dict_workcounts[auth][year]\n",
    "        except:\n",
    "            1\n",
    "# with open(path_career+'dict_authwc_after_year_x_STATPHY.pkl', 'wb') as f:\n",
    "with open(path_career+'dict_authwc_after_year_x.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authwc_after_year_x, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5b5ba-95fc-447a-98b3-922063defe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d4ead-bddf-4a51-a933-efc447b1cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee2fada-06a4-45bc-a59c-5cff82c45177",
   "metadata": {},
   "source": [
    "# dict auth_firstpub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050a1ccb-65bc-4dc5-8179-103c6d0f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2793035618960857\n",
      "None\n",
      "2.2804005183279514\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authorsfirstpubdate/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=4)\n",
    "df_firstpub = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_firstpub=df_firstpub.set_index('aid')['first_pub'].T.to_dict()\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_firstpub.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_firstpub, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345767-599d-4211-aa7d-bcb551c41c30",
   "metadata": {},
   "source": [
    "# dict_authage2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d506a074-0ddd-4582-a867-1dca7263ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_authage2023={x:2023- dict_firstpub[x] for x in dict_firstpub.keys()}\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_authage2023.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authage2023, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd37658-3f1c-45f1-95cc-b8a86f58e76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_firstpub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4e449-ba03-416a-bbcd-591fae621d69",
   "metadata": {},
   "source": [
    "# 2papersteamsizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bba119-ab35-4ed1-9ba2-5f331031a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.74788759276271\n",
      "None\n",
      "41.74942532926798\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2papersteamsizes/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=8)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_=df.set_index('paper_id')['f0_'].T.to_dict()\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_papersteamsizes.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bcbb4f3-6d7b-4bc0-9574-8b4f63abc6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112034640"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe72e0a-05a7-4ed1-887b-d33aeaa94498",
   "metadata": {},
   "source": [
    "# Author c5 per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcb4da-d6b4-403b-97d9-84aabd81353a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
