{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de858fd-073c-4d87-a2da-886c30df9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import timeit\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "def toc(start_time):\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import xmlschema\n",
    "from pprint import pprint\n",
    "import glob\n",
    "# importing element tree\n",
    "import lxml.etree as etree\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 8, 'font.style': 'normal', 'font.family':'serif'})\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    plt.xticks(fontsize=ff-4)\n",
    "    plt.yticks(fontsize=ff-4)\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7639f-6c63-429f-af74-130c2f4581bc",
   "metadata": {},
   "source": [
    "# Load authc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abbba19-eb59-4f9f-b781-897e1c5fa5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.708442833274603\n",
      "None\n",
      "1.7094787433743477\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authc5/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_authc5=df.set_index('auth_id')['c5'].T.to_dict()\n",
    "with open(path_career+'dict_authc5.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authc5, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348eb13-fae8-4a12-aa12-71dc393fc157",
   "metadata": {},
   "source": [
    "# Literature popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4673f9a-261c-407c-b36e-f436b602f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.52010865136981\n",
      "None\n",
      "25.521927036345005\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2literaturepopularity/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "literaturepopularity=df.set_index('citing')['c5_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturepopularity.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturepopularity, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaac0d5-3c90-446b-a929-a3b46bdc9616",
   "metadata": {},
   "source": [
    "# Auth Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493db441-12f5-4280-a0ec-3805b044d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d86a214-2ad2-4240-a9e4-46b165407e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.990139409899712\n",
      "None\n",
      "11.9905466735363\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2AuthNames/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "AuthNames=df.set_index('auth_id')['name'].T.to_dict()\n",
    "with open(path_career+'dict_AuthNames.pkl', 'wb') as f:\n",
    "    pickle.dump(AuthNames, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9672d-c8da-409d-bdb2-5bcac2a25544",
   "metadata": {},
   "source": [
    "# literature depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc69a4f1-3714-40f4-92c0-d9930db13a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.308283876627684\n",
      "None\n",
      "31.309843335300684\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2literaturedepth/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "literaturedepth=df.set_index('citing')['pubdate_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturedepth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturedepth, f) \n",
    "\n",
    "literaturebreadth=df.set_index('citing')['pubdate_std'].T.to_dict()\n",
    "with open(path_career+'dict_literaturebreadth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturebreadth, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccaa546-b7e7-4a3f-a96a-8ea39f0323c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "literaturedepth=df.set_index('citing')['pubdate_avg'].T.to_dict()\n",
    "with open(path_career+'dict_literaturedepth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturedepth, f) \n",
    "\n",
    "literaturebreadth=df.set_index('citing')['pubdate_std'].T.to_dict()\n",
    "with open(path_career+'dict_literaturebreadth.pkl', 'wb') as f:\n",
    "    pickle.dump(literaturebreadth, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924219b-52ca-44b9-b6cd-814de174a744",
   "metadata": {},
   "source": [
    "# Auth Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fab93a6-ed6c-4649-a83d-4c414378d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed83b21-65c3-4918-b106-3d5a6f8ba3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2AuthStats/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46934b82-6da7-4b62-b277-63d0f5ac77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "849714b0-06f7-41c3-ae17-4eccbc79fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [{'auth_id': 'https://openalex.org/A4338545609',\n",
    "#   'stats': {'works_count': '129',\n",
    "#    '2yr_h_index': '3',\n",
    "#    '2yr_i10_index': '0',\n",
    "#    '2yr_mean_citedness': 0.5454545454545454,\n",
    "#    'h_index': '27',\n",
    "#    '2yr_cited_by_count': '371',\n",
    "#    '2yr_works_count': '9',\n",
    "#    'cited_by_count': '3742',\n",
    "#    'i10_index': '40',\n",
    "#    'oa_percent': 17.05}},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b816662-2d4d-49fe-8e8d-9f060cec75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.0999\n",
      "3.0999\n",
      "4.0999\n",
      "5.0999\n",
      "6.0999\n",
      "7.0999\n",
      "8.0999\n",
      "9.0999\n",
      "100999\n",
      "110999\n",
      "120999\n",
      "130999\n",
      "140999\n",
      "150999\n",
      "160999\n",
      "170999\n",
      "180999\n",
      "190999\n",
      "200999\n",
      "210999\n",
      "220999\n",
      "230999\n",
      "240999\n",
      "250999\n",
      "260999\n",
      "270999\n",
      "280999\n",
      "290999\n",
      "300999\n",
      "310999\n",
      "320999\n",
      "330999\n",
      "340999\n",
      "350999\n",
      "360999\n",
      "370999\n",
      "380999\n",
      "390999\n",
      "400999\n",
      "410999\n",
      "420999\n",
      "430999\n",
      "440999\n",
      "450999\n",
      "460999\n",
      "470999\n",
      "480999\n",
      "490999\n",
      "500999\n",
      "510999\n",
      "520999\n",
      "530999\n",
      "540999\n",
      "550999\n",
      "560999\n",
      "570999\n",
      "580999\n",
      "590999\n",
      "600999\n",
      "1.0999\r"
     ]
    }
   ],
   "source": [
    "dict_AuthStats={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        dict_AuthStats[res['auth_id']]=res['stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b55229f-ff8e-45cb-9cec-811b64e96070",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_AuthStats.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_AuthStats, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a52da-c707-4843-a67b-6220ecb4d6a9",
   "metadata": {},
   "source": [
    "# Auth paper c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03c7515a-95cc-4af9-bc49-f917fa4868d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authpaperc5_FilteredOnUniqueCoreAuths/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc3682-3ec5-4b39-b74d-5fe46d422b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28c3ef7f-3c77-464f-99e8-1e4b574730b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0833\r"
     ]
    }
   ],
   "source": [
    "dict_AuthPaperC5={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;  L=len(result);it=0\n",
    "    print(str(round(IT/len(results),4)),end='\\r');\n",
    "\n",
    "    for res in result:\n",
    "#         it+=1;print(str(round(it/L,4)),end='\\r');\n",
    "        dict_AuthPaperC5[res['auth_id']]={p_:c5_ for p_,c5_ in zip(res['paper_id'],res['c5'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81d56913-c330-4174-ba79-ea38b61b0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_AuthPaperC5.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_AuthPaperC5, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4665697-f25c-40fc-93f0-cb70fe29ecd0",
   "metadata": {},
   "source": [
    "# Team author concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090fcbd8-e17d-4c45-b108-6ebb2ff9a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# names=path_career+'2authconcepts_filteredUniqueCore_grouped.json'\n",
    "# import json\n",
    " \n",
    "# # Opening JSON file\n",
    "# with open(names) as json_file:\n",
    "#     data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65aae128-f337-4dfe-99d7-478a6c796f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authconcepts_filteredUniqueCore_grouped*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fde5d4-3c31-4845-aabf-e2f3507a94b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e100e44c-3e88-4fc5-b025-5c8854be98bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auth_id': 'https://openalex.org/A2120662050',\n",
       " 'concepts': ['Medicine', 'Biology', 'Chemistry', 'Psychology', 'Geology'],\n",
       " 'score': [93.3, 80, 36.7, 26.7, 26.7]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b587a2fe-465a-4839-8603-7c58c897a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0999\r"
     ]
    }
   ],
   "source": [
    "dict_author_concepts={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        \n",
    "        dict_author_concepts[res['auth_id']]={c:s/np.sum(res['score']) for c,s in zip(res['concepts'],res['score'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a68c12-f6ea-4ea0-8ce2-bb6e88a74db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path_career+'dict_author_concepts.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_author_concepts, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7d7dc-f024-43bd-84e4-83d47db33992",
   "metadata": {},
   "source": [
    "# Team noncore author concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab3ecc7b-d110-4c0b-ad6b-5870226b1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authconcepts_noncore_FilterdUniqueNoncoreAuths/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc9a2f-a2da-43c4-95db-5282b78dd4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b8177ad-964b-49e6-99fa-ba8dca38858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.0999\n",
      "3.0999\n",
      "4.0999\n",
      "5.0999\n",
      "6.0999\n",
      "1.0999\r"
     ]
    }
   ],
   "source": [
    "dict_author_concepts={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        \n",
    "        dict_author_concepts[res['auth_id']]={c:s/np.sum(res['score']) for c,s in zip(res['concepts'],res['score'])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d39c4915-8bf7-4fa4-984c-779b8f93fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_author_concepts_noncore.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_author_concepts, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484965cf-4f8b-4e8c-835f-240a9c1e1fc6",
   "metadata": {},
   "source": [
    "# Team author countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552f0aa6-6582-4334-a596-c78693c2ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2paperauthaffilsgrouped/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f50272-1405-4dc0-8f4f-c38607d6dee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93d33d-9a86-4ac8-afc5-9fc31f73d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.0999\n",
      "0.3136\r"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_paper_auth_geo={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        if len(res)==3:\n",
    "            if res['paper_id'] not in dict_paper_auth_geo.keys():\n",
    "                dict_paper_auth_geo[res['paper_id']]={}\n",
    "            try:\n",
    "                dict_paper_auth_geo[res['paper_id']][res['auth_id']]=res['countries']\n",
    "            except:\n",
    "                dict_paper_auth_geo[res['paper_id']]={}\n",
    "                dict_paper_auth_geo[res['paper_id']][res['auth_id']]=res['countries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b3f1e-8dd6-4894-b4e3-1f1001c5fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_career+'dict_paper_auth_geo.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_paper_auth_geo, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b28bc-75e2-45b8-9474-55893774ef4d",
   "metadata": {},
   "source": [
    "# Team author affiliations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d55f51f-bb4e-4184-88cb-0ca2e387fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authpaperuniversity_filterUniqueCorePapers_grouped/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa0eb3e-905b-4daa-936a-ac38103b5a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': 'https://openalex.org/W2020636712',\n",
       " 'auth_id': 'https://openalex.org/A2583380327',\n",
       " 'f0_': ['https://openalex.org/I102322142']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e279c5-67dd-4f03-91d7-be03c2f8347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.0999\n",
      "3.0999\n",
      "4.0999\n",
      "5.0999\n",
      "6.0999\n",
      "7.0999\n",
      "8.0999\n",
      "9.0999\n",
      "100999\n",
      "110999\n",
      "120999\n",
      "130999\n",
      "140999\n",
      "150999\n",
      "160999\n",
      "170999\n",
      "180999\n",
      "190999\n",
      "200999\n",
      "210999\n",
      "220999\n",
      "230999\n",
      "240999\n",
      "250999\n",
      "260999\n",
      "270999\n",
      "280999\n",
      "290999\n",
      "300999\n",
      "310999\n",
      "320999\n",
      "330999\n",
      "340999\n",
      "350999\n",
      "360999\n",
      "370999\n",
      "380999\n",
      "390999\n",
      "400999\n",
      "410999\n",
      "420999\n",
      "430999\n",
      "440999\n",
      "450999\n",
      "460999\n",
      "470999\n",
      "480999\n",
      "490999\n",
      "500999\n",
      "510999\n",
      "520999\n",
      "530999\n",
      "540999\n",
      "550999\n",
      "560999\n",
      "570999\n",
      "580999\n",
      "590999\n",
      "600999\n",
      "610999\n",
      "620999\n",
      "630999\n",
      "640999\n",
      "650999\n",
      "660999\n",
      "670999\n",
      "680999\n",
      "690999\n",
      "700999\n",
      "710999\n",
      "720999\n",
      "730999\n",
      "740999\n",
      "750999\n",
      "760999\n",
      "770999\n",
      "780999\n",
      "790999\n",
      "800999\n",
      "810999\n",
      "820999\n",
      "830999\n",
      "840999\n",
      "850999\n",
      "860999\n",
      "870999\n",
      "880999\n",
      "890999\n",
      "900999\n",
      "1.0999\r"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_paper_auth_afill={}\n",
    "IT=0\n",
    "for result in results:\n",
    "    IT+=1;    L=len(result);it=0\n",
    "    print(IT);\n",
    "    for res in result:\n",
    "        it+=1\n",
    "        print(str(round(it/L,4)),end='\\r');\n",
    "        if len(res)==3:\n",
    "            if res['paper_id'] not in dict_paper_auth_afill.keys():\n",
    "                dict_paper_auth_afill[res['paper_id']]={}\n",
    "            try:\n",
    "                dict_paper_auth_afill[res['paper_id']][res['auth_id']]=res['f0_']\n",
    "            except:\n",
    "                dict_paper_auth_afill[res['paper_id']]={}\n",
    "                dict_paper_auth_afill[res['paper_id']][res['auth_id']]=res['f0_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20809c08-dac0-4009-bfe8-d8ef96d18ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_paper_auth_university.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_paper_auth_afill, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1727f97b-0dd9-43b4-b303-6d972fb9fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bccb3f-da7f-4352-8664-cde727a10471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2c85-0df0-4d3c-a8bb-a936926da550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "702c13e3-8a7e-48af-b2da-b8b0eef48384",
   "metadata": {},
   "source": [
    "# country to continent dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0fd5f2-eab7-47e9-9f77-af14e65c495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "\n",
    "# Create a dictionary to map alpha-2 country codes to continents\n",
    "country_to_continent = {}\n",
    "for country in pycountry.countries:\n",
    "    alpha_2_code = country.alpha_2\n",
    "    try:\n",
    "        continent_code = country_alpha2_to_continent_code(alpha_2_code)\n",
    "        # Convert continent code to continent name\n",
    "        continent_name=pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        country_to_continent[alpha_2_code] = continent_name\n",
    "    except KeyError:\n",
    "        # Some countries may not have continent information\n",
    "        pass\n",
    "\n",
    "# Example: Accessing the continent for a specific country code (e.g., 'US' for United States)\n",
    "country_code = 'US'\n",
    "if country_code in country_to_continent:\n",
    "    continent = country_to_continent[country_code]\n",
    "    \n",
    "    print(f\"The continent of {country_code} is {continent}\")\n",
    "else:\n",
    "    print(f\"Continent information not found for {country_code}\")\n",
    "\n",
    "# Example: Accessing the continent for all countries\n",
    "# for country_code, continent in country_to_continent.items():\n",
    "#     print(f\"{country_code}: {continent}\")\n",
    "\n",
    "with open(path_career+'country_to_continent.pkl', 'wb') as f:\n",
    "    pickle.dump(country_to_continent, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823c8d1-422a-442a-b45e-d6497b712a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The continent of US is North America\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2533e5-ce44-40bc-823f-83d8eb256be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883608b1-fe7d-4355-ac11-a97ef056c552",
   "metadata": {},
   "source": [
    "# Load pairs_first_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ec293-e1f4-4d4c-9f58-8e209281693f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fe8510883a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sdb1/sandeep/miniconda3/envs/sos/lib/python3.9/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authpairsfirstlastpub/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=15)\n",
    "# df_workcounts = pd.read_csv(files)\n",
    "df_pairs_firstpubtogether = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "\n",
    "# p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "# df_pairs_firstpubtogether[\"A1A2\"] = df_pairs_firstpubtogether[\"A1\"] +'+'+ df_pairs_firstpubtogether[\"A2\"]\n",
    "# dict_pairs_firstpubtogether = df_pairs_firstpubtogether.set_index('A1A2')[['first_year_together']].to_dict()\n",
    "dict_pairs_firstpubtogether = df_pairs_firstpubtogether.groupby('A1')[['A2','first_year_together','last_year_together']].apply(lambda x: x.set_index('A2').to_dict(orient='index')).to_dict()\n",
    "with open(path_career+'dict_pairs_firstpubtogether.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_pairs_firstpubtogether, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc49876-f84e-4b27-9776-23eeb4867e0c",
   "metadata": {},
   "source": [
    "# auth workcounts total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98737034-cf72-495f-afc2-ef25e396b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.81174374371767\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authworkcounts/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=3)\n",
    "# df_workcounts = pd.read_csv(files)\n",
    "df_workcounts = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "\n",
    "# p.close()\n",
    "\n",
    "print(toc(start_time))\n",
    "\n",
    "\n",
    "dict_workcounts=df_workcounts.set_index('auth_id')['wc'].T.to_dict()\n",
    "with open(path_career+'dict_workcounts.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_workcounts, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fad15-40a6-49b0-9566-c1ab89a523b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf60f31-0f90-4582-b7ba-310e7c46341a",
   "metadata": {},
   "source": [
    "# authors to papers dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec396b1-16ec-499a-85bb-f03e3e7528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "names=path_career+'2authorsgroupedpapers/*'\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# json_files = [f for f in os.listdir('.') if f.endswith('.json')]\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "# Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n",
    "\n",
    "dict_auth_to_paper={}\n",
    "for result in results:\n",
    "    for res in result:\n",
    "        dict_auth_to_paper[res['auth_id']]=res['papers']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e6e42-8d94-4b51-9801-a3c42e95b499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f2b3f-8f8f-4764-a024-4c22216e7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_career+'dict_auth_to_paper(whole).pkl', 'wb') as f:\n",
    "    pickle.dump(dict_auth_to_paper, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d8823-94fa-45ae-8c59-b430e3cc0a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e097ae65-ffeb-4933-a1c0-3029e13ff9a3",
   "metadata": {},
   "source": [
    "# c5\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0273b39f-901c-42ac-8d77-3c9154f3287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.530694387853146\n",
      "None\n",
      "21.531843692064285\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2c5norm/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df_c5_norm = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_c5_norm=df_c5_norm.set_index('id')['c5_normalized'].T.to_dict()\n",
    "with open(path_career+'dict_c5_norm.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_c5_norm, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616a122-5e44-477d-998d-cfa57ac85e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8550892a-dec1-47cb-95d3-5109079648bc",
   "metadata": {},
   "source": [
    "# paper pubdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f966eff-0429-4b88-a4fd-b76f832f76f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.554304394871\n",
      "None\n",
      "106.55620150640607\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2paperspubdates/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=5)\n",
    "df_pubdate = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "# names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# df.to_parquet(names_par,index=None)\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "\n",
    "dict_pubdate=df_pubdate.set_index('id')['pubdate'].T.to_dict()\n",
    "with open(path_career+'dict_pubdate.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_pubdate, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8ae0f-7aae-401f-abc9-b19c154c923f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dca7977-d4f4-4002-b938-c0b4a72bf5c6",
   "metadata": {},
   "source": [
    "# disciplinary diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5507a68f-84b5-4073-8be7-1b3374c6449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.0606624558568\n",
      "None\n",
      "108.0622975602746\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2papersconceptsdiversity/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=15)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "# names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# df.to_parquet(names_par,index=None)\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_=df.set_index('paper_id')['concept_entropy'].T.to_dict()\n",
    "with open(path_career+'dict_paper_to_discidiversity.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225ecf4-9a52-4f00-a5c2-e5b02e1303d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8dd2782-715c-49c4-b123-7edc1bb0bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_=df.set_index('paper_id')['concept_entropy'].T.to_dict()\n",
    "with open(path_career+'dict_paper_to_discidiversity.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38561e2a-0e1b-47cc-a4e5-9543d980691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70abe883-3bbe-419a-9080-d39c1bc73eed",
   "metadata": {},
   "source": [
    "# c5 within discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5cb6c85-986b-453d-9df1-06376f72a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # \n",
    "# start_time = timeit.default_timer()\n",
    "# names=path_career+'c5_within_discipline/*'\n",
    "# files=sorted(glob.glob(names))\n",
    "\n",
    "# p=Pool(processes=5)\n",
    "# df_c5_within = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "# print(toc(start_time))\n",
    "# # names_par=\"/mnt/sdb1/sandeep/open_alex_ACTIV/df_paper_auth.parquet\"\n",
    "# # df.to_parquet(names_par,index=None)\n",
    "\n",
    "# p.close()\n",
    "# print(toc(start_time))\n",
    "\n",
    "# dict_c5_within=df_c5_within.set_index('cited')['c5'].T.to_dict()\n",
    "# with open(path_career+'dict_c5_within.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_c5_within, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ebb16b-9526-4f1c-b41b-79de700daaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # \n",
    "# start_time = timeit.default_timer()\n",
    "# names=path_career+'c5_within_lvl1/*'\n",
    "# files=sorted(glob.glob(names))\n",
    "\n",
    "# p=Pool(processes=5)\n",
    "# df_c5_within_lvl1 = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "# print(toc(start_time))\n",
    "\n",
    "\n",
    "# p.close()\n",
    "# print(toc(start_time))\n",
    "\n",
    "# dict_c5_within_lvl1=df_c5_within_lvl1.set_index('cited')['c5'].T.to_dict()\n",
    "# with open(path_career+'dict_c5_within_lvl1.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_c5_within_lvl1, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d5ea9-003e-46a6-91e8-771a5e76568d",
   "metadata": {},
   "source": [
    "# workcount_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ea9aa8-c2ce-4d6a-b79a-ef3563cd71da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb550122-5493-4bb6-bd46-dd64604ea9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# List all the JSON files in the current directory\n",
    "# names=path_career+'2workcountsperyear_statphy/*'\n",
    "names=path_career+'2workcounts_per_year_for_authors/*'\n",
    "\n",
    "json_files=sorted(glob.glob(names))\n",
    "\n",
    "# json_files = [f for f in os.listdir('.') if f.endswith('.json')]\n",
    "\n",
    "def process_data(data):\n",
    "    return data\n",
    "\n",
    "def process_json_file(filename):\n",
    "    data=[]\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        # Process the data here...\n",
    "        processed_data = process_data(data)\n",
    "        return processed_data\n",
    "\n",
    "\n",
    "    # Create a pool of workers to process the files concurrently\n",
    "with Pool() as pool:\n",
    "    # Apply the processing function to each JSON file concurrently\n",
    "    results = pool.map(process_json_file, json_files)\n",
    "\n",
    "dict_workcounts={}\n",
    "for result in results:\n",
    "    for res in result:\n",
    "        dict_workcounts[res['auth_id']]={int(year):int(wc) for year,wc in sorted(zip(res['pubyear'],res['wc']))}\n",
    "\n",
    "# with open(path_career+'dict_workcounts_per_year_for_authors_statphy.pkl', 'wb') as f:\n",
    "with open(path_career+'dict_workcounts_per_year_for_authors.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_workcounts, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4496ee0-4ba8-40d2-ac7c-449efc69be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_authwc_after_year_x={}\n",
    "for auth in dict_workcounts.keys():\n",
    "    dict_authwc_after_year_x[auth]={}\n",
    "    totalworks=np.sum(list(dict_workcounts[auth].values()))\n",
    "    wcount=0\n",
    "    for year in range(np.min(list(dict_workcounts[auth].keys())),2024):\n",
    "\n",
    "        dict_authwc_after_year_x[auth][year]=totalworks-wcount\n",
    "        try:\n",
    "            wcount+=dict_workcounts[auth][year]\n",
    "        except:\n",
    "            1\n",
    "# with open(path_career+'dict_authwc_after_year_x_STATPHY.pkl', 'wb') as f:\n",
    "with open(path_career+'dict_authwc_after_year_x.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authwc_after_year_x, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5b5ba-95fc-447a-98b3-922063defe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d4ead-bddf-4a51-a933-efc447b1cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee2fada-06a4-45bc-a59c-5cff82c45177",
   "metadata": {},
   "source": [
    "# dict auth_firstpub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050a1ccb-65bc-4dc5-8179-103c6d0f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2793035618960857\n",
      "None\n",
      "2.2804005183279514\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2authorsfirstpubdate/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=4)\n",
    "df_firstpub = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_firstpub=df_firstpub.set_index('aid')['first_pub'].T.to_dict()\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_firstpub.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_firstpub, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345767-599d-4211-aa7d-bcb551c41c30",
   "metadata": {},
   "source": [
    "# dict_authage2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d506a074-0ddd-4582-a867-1dca7263ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_authage2023={x:2023- dict_firstpub[x] for x in dict_firstpub.keys()}\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_authage2023.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_authage2023, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd37658-3f1c-45f1-95cc-b8a86f58e76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_firstpub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4e449-ba03-416a-bbcd-591fae621d69",
   "metadata": {},
   "source": [
    "# 2papersteamsizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bba119-ab35-4ed1-9ba2-5f331031a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.74788759276271\n",
      "None\n",
      "41.74942532926798\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # \n",
    "start_time = timeit.default_timer()\n",
    "names=path_career+'2papersteamsizes/*'\n",
    "files=sorted(glob.glob(names))\n",
    "\n",
    "p=Pool(processes=8)\n",
    "df = pd.concat(p.map(pd.read_csv,files),ignore_index=True)\n",
    "print(toc(start_time))\n",
    "\n",
    "p.close()\n",
    "print(toc(start_time))\n",
    "\n",
    "dict_=df.set_index('paper_id')['f0_'].T.to_dict()\n",
    "\n",
    "path_career='/mnt/sdb1/sandeep/openalex_ACTIV/'\n",
    "import pickle\n",
    "with open(path_career+'dict_papersteamsizes.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bcbb4f3-6d7b-4bc0-9574-8b4f63abc6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112034640"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe72e0a-05a7-4ed1-887b-d33aeaa94498",
   "metadata": {},
   "source": [
    "# Author c5 per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcb4da-d6b4-403b-97d9-84aabd81353a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
